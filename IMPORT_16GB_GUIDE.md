# üì¶ Guide Import 16 GB / 4-5M Entreprises

## üéØ Optimisations Impl√©ment√©es

### ‚úÖ Ce qui a √©t√© am√©lior√©

1. **Streaming m√©moire** - Ne charge jamais tout le fichier en RAM
2. **Batch size augment√©** - 5000 au lieu de 1000 (optimal pour gros volumes)
3. **Buffer I/O optimis√©** - Buffering adaptatif selon taille fichier
4. **CSV field size illimit√©** - Support de lignes tr√®s longues
5. **Mode raw SQL** - Insertion directe PostgreSQL (COPY FROM) avec `--no-validation`
6. **Reprise d'import** - `--skip-rows` pour reprendre apr√®s crash
7. **Statistiques temps r√©el** - Affichage toutes les 10s (pas de flood)

---

## üöÄ Utilisation

### 1. Pr√©paration (CRITIQUE)

```bash
# V√©rifier l'espace disque
df -h  # Besoin de ~25 GB libres (fichier 16GB + tables ~9GB)

# V√©rifier RAM PostgreSQL
psql -U postgres -c "SHOW shared_buffers;"
# Recommand√©: 2GB minimum

# D√©sactiver les contraintes temporairement (optionnel, +20% vitesse)
psql -U postgres -d foxreviews << EOF
ALTER TABLE enterprise_entreprise DISABLE TRIGGER ALL;
ALTER TABLE enterprise_entreprise DROP CONSTRAINT IF EXISTS enterprise_entreprise_siren_unique;
EOF
```

### 2. Test avec 100K lignes d'abord

```bash
# Toujours tester avec un petit √©chantillon
uv run python manage.py import_entreprises_bulk data/entreprises.csv \
    --max-rows 100000 \
    --batch-size 5000 \
    --dry-run

# Puis vraiment importer les 100K
uv run python manage.py import_entreprises_bulk data/entreprises.csv \
    --max-rows 100000 \
    --batch-size 5000
```

### 3. Import COMPLET (4-5M entreprises)

#### Option A: Mode Standard (avec validation Django)

```bash
# Dur√©e estim√©e: 3-5 heures
# D√©bit: ~300-500 rows/sec
uv run python manage.py import_entreprises_bulk data/entreprises.csv \
    --batch-size 5000
```

#### Option B: Mode ULTRA-RAPIDE (sans validation, raw SQL)

```bash
# Dur√©e estim√©e: 1-2 heures
# D√©bit: ~1000-2000 rows/sec
# ‚ö†Ô∏è RISQUE: Pas de validation Django, donn√©es doivent √™tre propres
uv run python manage.py import_entreprises_bulk data/entreprises.csv \
    --batch-size 10000 \
    --no-validation
```

### 4. Reprise apr√®s crash

```bash
# Si import crash √† 2M lignes, reprendre √† partir de l√†
uv run python manage.py import_entreprises_bulk data/entreprises.csv \
    --skip-rows 2000000 \
    --batch-size 5000
```

---

## üìä Performance Attendue

### Configuration Minimale
- **CPU:** 4 cores
- **RAM:** 8 GB (Django) + 4 GB (PostgreSQL)
- **Disque:** SSD recommand√©
- **D√©bit:** 300-500 rows/sec
- **Dur√©e:** 3-5 heures pour 5M lignes

### Configuration Optimale
- **CPU:** 8+ cores
- **RAM:** 16 GB (Django) + 8 GB (PostgreSQL)
- **Disque:** NVMe SSD
- **D√©bit:** 1000-2000 rows/sec (avec `--no-validation`)
- **Dur√©e:** 1-2 heures pour 5M lignes

---

## üîß Optimisations PostgreSQL

### Avant l'import

```sql
-- Augmenter la m√©moire de travail
ALTER SYSTEM SET work_mem = '256MB';
ALTER SYSTEM SET maintenance_work_mem = '1GB';
ALTER SYSTEM SET shared_buffers = '2GB';

-- D√©sactiver autovacuum pendant import
ALTER TABLE enterprise_entreprise SET (autovacuum_enabled = false);

-- D√©sactiver WAL archiving (si applicable)
ALTER SYSTEM SET wal_level = 'minimal';
ALTER SYSTEM SET max_wal_senders = 0;

-- Recharger config
SELECT pg_reload_conf();
```

### Apr√®s l'import

```sql
-- R√©activer autovacuum
ALTER TABLE enterprise_entreprise SET (autovacuum_enabled = true);

-- VACUUM ANALYZE complet
VACUUM ANALYZE VERBOSE enterprise_entreprise;

-- REINDEX pour reconstruire tous les index
REINDEX TABLE CONCURRENTLY enterprise_entreprise;

-- V√©rifier les stats
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
    n_live_tup as rows
FROM pg_stat_user_tables
WHERE tablename = 'enterprise_entreprise';

-- Restaurer les param√®tres
ALTER SYSTEM RESET work_mem;
ALTER SYSTEM RESET maintenance_work_mem;
ALTER SYSTEM RESET shared_buffers;
SELECT pg_reload_conf();
```

---

## üìù Format CSV Requis

### Colonnes obligatoires

```csv
siren,nom,adresse,code_postal,ville_nom,naf_code
123456789,SARL TEST,123 Rue Test,75001,Paris,6201Z
```

### Colonnes optionnelles

```csv
siren,siret,nom,nom_commercial,adresse,code_postal,ville_nom,naf_code,naf_libelle,telephone,email,site_web
123456789,12345678900001,SARL TEST,Test Company,123 Rue Test,75001,Paris,6201Z,Programmation informatique,0123456789,test@example.com,https://example.com
```

### Contraintes
- **SIREN:** 9 chiffres exactement
- **SIRET:** 14 chiffres max
- **Nom:** 255 caract√®res max
- **Encoding:** UTF-8 obligatoire
- **S√©parateur:** `,` (virgule)
- **Quote:** `"` pour champs avec virgules

---

## ‚ö†Ô∏è Probl√®mes Courants

### 1. "Out of Memory"

**Sympt√¥me:** Python crash avec `MemoryError`

**Solutions:**
```bash
# R√©duire batch size
--batch-size 2000

# Augmenter swap
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### 2. "Too many connections PostgreSQL"

**Sympt√¥me:** `FATAL: sorry, too many clients already`

**Solution:**
```sql
ALTER SYSTEM SET max_connections = 200;
SELECT pg_reload_conf();
```

### 3. Import tr√®s lent (< 100 rows/sec)

**Causes possibles:**
- Disque HDD au lieu de SSD
- Index pas d√©sactiv√©s
- Contraintes foreign key actives
- PostgreSQL mal configur√©

**Solutions:**
```bash
# Utiliser --no-validation
--no-validation

# V√©rifier EXPLAIN ANALYZE
psql -U postgres -d foxreviews -c "EXPLAIN ANALYZE SELECT * FROM enterprise_entreprise LIMIT 1;"
```

### 4. CSV mal encod√©

**Sympt√¥me:** `UnicodeDecodeError`

**Solutions:**
```bash
# V√©rifier l'encoding
file -i data/entreprises.csv

# Convertir en UTF-8 si besoin
iconv -f ISO-8859-1 -t UTF-8 data/entreprises.csv > data/entreprises_utf8.csv
```

---

## üéØ Checklist Avant Import Production

- [ ] Backup complet de la base de donn√©es
- [ ] Espace disque suffisant (25+ GB)
- [ ] PostgreSQL configur√© (shared_buffers, work_mem)
- [ ] Test r√©ussi avec 100K lignes
- [ ] D√©sactivation contraintes/triggers (optionnel)
- [ ] Monitoring serveur actif (CPU, RAM, disque)
- [ ] Plan de reprise en cas d'√©chec (`--skip-rows`)
- [ ] Cr√©neaux horaires d√©finis (import hors heures de pointe)

---

## üìà Monitoring en temps r√©el

### Terminal 1: Import

```bash
uv run python manage.py import_entreprises_bulk data/entreprises.csv \
    --batch-size 5000 \
    --no-validation
```

### Terminal 2: Stats PostgreSQL

```bash
# Voir nombre de lignes en temps r√©el
watch -n 5 'psql -U postgres -d foxreviews -c "SELECT COUNT(*) FROM enterprise_entreprise;"'

# Voir taille table
watch -n 10 'psql -U postgres -d foxreviews -c "SELECT pg_size_pretty(pg_total_relation_size('\''enterprise_entreprise'\''));"'

# Voir activit√©
watch -n 2 'psql -U postgres -d foxreviews -c "SELECT * FROM pg_stat_activity WHERE datname='\''foxreviews'\'';"'
```

### Terminal 3: Ressources syst√®me

```bash
# CPU, RAM, Disque
htop

# I/O disque
iostat -x 2
```

---

## üèÜ R√©sultat Final Attendu

```
======================================================================
‚úÖ IMPORT TERMIN√â
======================================================================
‚úÖ Import√©es:    4,850,000 entreprises
‚ùå Erreurs:         12,543 lignes (0.26%)
‚è±Ô∏è Dur√©e:        01h 45m 32s
üìä D√©bit:            768 rows/s
üíæ Donn√©es:       ~2.3 GB
üíæ Total DB:    4,850,000 entreprises
======================================================================
```

---

## üîó Fichiers Li√©s

- [import_entreprises_bulk.py](foxreviews/enterprise/management/commands/import_entreprises_bulk.py) - Commande d'import
- [SCALING_4M_ENTREPRISES.sql](SCALING_4M_ENTREPRISES.sql) - Indexes SQL
- [IMPORT_4M_GUIDE.md](IMPORT_4M_GUIDE.md) - Guide original
- [DATABASE_OPTIMIZATION.md](DATABASE_OPTIMIZATION.md) - Optimisations DB

---

**Date:** 22 d√©cembre 2025  
**Version:** 2.0 (Support 16 GB CSV)
