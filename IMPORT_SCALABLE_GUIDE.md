# ğŸš€ Import Scalable - Millions d'Entreprises INSEE

## ğŸ¯ Version OptimisÃ©e pour MILLIONS d'entreprises

Cette commande utilise des **techniques de scalabilitÃ© avancÃ©es** pour importer jusqu'Ã  **10-12 millions d'entreprises** en quelques heures.

---

## âš¡ Optimisations ImplÃ©mentÃ©es

| Technique | Gain | Description |
|-----------|------|-------------|
| **Bulk Insert** | **1000x** | `bulk_create()` au lieu de `save()` individuel |
| **Cache MÃ©moire** | **100x** | Villes, sous-catÃ©gories, SIREN en RAM |
| **Transactions par Batch** | **50x** | Commit toutes les 1000 lignes |
| **Checkpoints** | âˆ | Reprendre en cas d'Ã©chec |
| **Ignore Conflicts** | **10x** | Skip doublons sans erreur |

### Comparaison Performance

| MÃ©thode | Temps (1M entreprises) | RequÃªtes DB |
|---------|------------------------|-------------|
| **Ancienne** (save individuel) | ~10-12 heures | 2,000,000+ |
| **Nouvelle** (bulk optimisÃ©) | **~30-45 min** | **~2,000** |

---

## ğŸš€ Utilisation

### Mode 1 : Import Complet (RecommandÃ©)
```bash
# Import de TOUTES les entreprises franÃ§aises (~10-12 millions)
# DurÃ©e estimÃ©e : 3-6 heures
docker exec -d foxreviews_local_django python manage.py import_insee_scalable --batch-size 1000 > /tmp/import_scalable.log 2>&1

# Suivre la progression
docker exec foxreviews_local_django tail -f /tmp/import_scalable.log
```

### Mode 2 : Test avec Limite
```bash
# Test avec 10k entreprises par dÃ©partement (1-2h)
docker exec foxreviews_local_django python manage.py import_insee_scalable --limit-per-dept 10000 --batch-size 1000
```

### Mode 3 : DÃ©partements CiblÃ©s
```bash
# Import Ãle-de-France uniquement (75, 92, 93, 94, 95, 77, 78, 91)
docker exec foxreviews_local_django python manage.py import_insee_scalable --departements 75,92,93,94,95,77,78,91 --batch-size 1000
```

### Mode 4 : Reprendre aprÃ¨s Interruption
```bash
# Si l'import a Ã©tÃ© interrompu, reprendre oÃ¹ il s'est arrÃªtÃ©
docker exec foxreviews_local_django python manage.py import_insee_scalable --resume --batch-size 1000
```

### Mode 5 : Sans ProLocalisations (Plus Rapide)
```bash
# Import entreprises seulement, sans ProLocalisations (2x plus rapide)
docker exec foxreviews_local_django python manage.py import_insee_scalable --skip-proloc --batch-size 1000
```

---

## ğŸ“Š Workflow Complet

### Ã‰tape 1 : PrÃ©paration
```bash
# VÃ©rifier l'espace disque disponible (50-100 GB recommandÃ©)
docker exec foxreviews_local_django df -h

# VÃ©rifier les villes en base
docker exec foxreviews_local_django python manage.py shell -c "from foxreviews.location.models import Ville; print(f'{Ville.objects.count():,} villes')"
```

### Ã‰tape 2 : Lancer l'Import
```bash
# Import complet en arriÃ¨re-plan
docker exec -d foxreviews_local_django python manage.py import_insee_scalable --batch-size 1000 > /tmp/import_scalable.log 2>&1
```

### Ã‰tape 3 : Monitoring
```bash
# Suivre les logs en temps rÃ©el
docker exec foxreviews_local_django tail -f /tmp/import_scalable.log

# Compter les entreprises en base
docker exec foxreviews_local_django python manage.py shell -c "from foxreviews.enterprise.models import Entreprise; print(f'{Entreprise.objects.count():,} entreprises')"

# VÃ©rifier le checkpoint
docker exec foxreviews_local_django cat logs/import_checkpoint.json
```

### Ã‰tape 4 : AprÃ¨s l'Import
```bash
# 1. CrÃ©er les catÃ©gories manquantes
docker exec foxreviews_local_django python manage.py create_categories_from_insee --top 1000 --update-mapping

# 2. CrÃ©er les ProLocalisations manquantes (si --skip-proloc utilisÃ©)
docker exec foxreviews_local_django python manage.py create_missing_prolocalisations

# 3. GÃ©nÃ©rer le contenu IA (en arriÃ¨re-plan)
docker exec -d foxreviews_local_django python manage.py generate_ai_reviews_v2 --batch-size 1000
```

---

## ğŸ” DÃ©tails Techniques

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API INSEE (30 req/min, pagination 1000)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cache MÃ©moire (chargÃ© au dÃ©marrage)       â”‚
â”‚  â€¢ 35k+ villes                             â”‚
â”‚  â€¢ 150+ sous-catÃ©gories                    â”‚
â”‚  â€¢ 91k+ SIREN existants                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Traitement par Batch (1000 entreprises)   â”‚
â”‚  â€¢ Extract data                            â”‚
â”‚  â€¢ Filter existants (cache)                â”‚
â”‚  â€¢ Prepare ProLocalisations                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bulk Insert (transaction atomique)        â”‚
â”‚  â€¢ Entreprise.bulk_create()                â”‚
â”‚  â€¢ ProLocalisation.bulk_create()           â”‚
â”‚  â€¢ ignore_conflicts=True                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Checkpoint (JSON file)                    â”‚
â”‚  â€¢ DÃ©partement en cours                    â”‚
â”‚  â€¢ DÃ©partements terminÃ©s                   â”‚
â”‚  â€¢ Stats cumulÃ©es                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimisations MÃ©moire

```python
# Cache intelligent : seulement ce qui est nÃ©cessaire
cache_villes = {
    ("paris", "75001"): Ville(id=1, ...),
    ("lyon", "69001"): Ville(id=2, ...),
    # ~35k entrÃ©es Ã— 500 bytes = ~17 MB
}

cache_sous_categories = {
    "62.01Z": SousCategorie(id=1, ...),
    "43.22A": SousCategorie(id=2, ...),
    # ~150 entrÃ©es Ã— 300 bytes = ~45 KB
}

cache_siren_existants = {
    "123456789",
    "987654321",
    # ~91k entrÃ©es Ã— 20 bytes = ~1.8 MB
}

# Total mÃ©moire cache : ~20 MB (nÃ©gligeable)
```

### Bulk Insert Performance

```python
# Ancien code (LENT)
for entreprise_data in batch:
    entreprise = Entreprise.objects.create(**entreprise_data)  # 1 query Ã— 1000
# â†’ 1000 queries SQL, ~10-15 secondes

# Nouveau code (RAPIDE)
to_create = [Entreprise(**data) for data in batch]
Entreprise.objects.bulk_create(to_create, batch_size=1000)  # 1 query
# â†’ 1 query SQL, ~0.1 seconde (100x plus rapide)
```

---

## ğŸ“ˆ Estimation Import Complet

### Configuration RecommandÃ©e
- **Batch Size**: 1000
- **Environnement**: Production (4 CPU, 8 GB RAM)
- **RÃ©seau**: Stable

### Temps EstimÃ©s

| DÃ©partements | Entreprises | DurÃ©e | RequÃªtes API |
|-------------|-------------|-------|--------------|
| 1 (Paris 75) | ~300k | 15-20 min | ~300 |
| 8 (Ãle-de-France) | ~2M | 1-2h | ~2,000 |
| 101 (France entiÃ¨re) | ~10-12M | **3-6h** | ~12,000 |

### Vitesse Moyenne
- **Sans ProLocalisations**: 1000-1500 entreprises/seconde
- **Avec ProLocalisations**: 500-800 entreprises/seconde

---

## âš ï¸ Limitations et ConsidÃ©rations

### 1. Quota API INSEE
- **Limite**: 30 requÃªtes/minute
- **Gestion**: Retry automatique avec backoff
- **Impact**: Ajoute ~2 secondes entre dÃ©partements

### 2. Espace Disque
| DonnÃ©es | Taille | DÃ©tails |
|---------|--------|---------|
| **Entreprises** (10M) | ~30 GB | Table principale |
| **ProLocalisations** (50M) | ~80 GB | Relations |
| **Indexes** | ~20 GB | Performance |
| **Total** | **~130 GB** | PrÃ©voir 150 GB min |

### 3. MÃ©moire RAM
- **Minimum**: 4 GB
- **RecommandÃ©**: 8 GB
- **Cache total**: ~20 MB (nÃ©gligeable)

### 4. PostgreSQL
```sql
-- Optimisations recommandÃ©es (postgresql.conf)
shared_buffers = 2GB
work_mem = 50MB
maintenance_work_mem = 512MB
effective_cache_size = 6GB
max_wal_size = 4GB
```

---

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨me 1 : "Out of Memory"
**Solution**:
```bash
# RÃ©duire la taille des batches
docker exec foxreviews_local_django python manage.py import_insee_scalable --batch-size 500
```

### ProblÃ¨me 2 : "Quota API dÃ©passÃ©"
**Solution**:
```bash
# Attendre 1 minute et utiliser --resume
sleep 60
docker exec foxreviews_local_django python manage.py import_insee_scalable --resume
```

### ProblÃ¨me 3 : Import interrompu
**Solution**:
```bash
# Reprendre automatiquement depuis le checkpoint
docker exec foxreviews_local_django python manage.py import_insee_scalable --resume
```

### ProblÃ¨me 4 : Trop lent
**Solution**:
```bash
# VÃ©rifier les indexes PostgreSQL
docker exec foxreviews_local_postgres psql -U foxreviews -c "\d+ enterprise_entreprise"

# RecrÃ©er les indexes si manquants
docker exec foxreviews_local_django python manage.py migrate --run-syncdb
```

---

## ğŸ“Š MÃ©triques de SuccÃ¨s

### Avant
```
ğŸ“Š Entreprises: 91,957
â±ï¸  Import: ~3-4 heures (avec limites)
ğŸ’¾ Espace: ~5 GB
```

### AprÃ¨s (Import Complet)
```
ğŸ“Š Entreprises: 10,000,000+ (France entiÃ¨re)
â±ï¸  Import: 3-6 heures (optimisÃ©)
ğŸ’¾ Espace: ~130 GB
âš¡ Vitesse: 500-1500 entreprises/seconde
âœ… ProLocalisations: 50,000,000+
ğŸ¯ Couverture: 100%
```

---

## âœ… Checklist Post-Import

- [ ] VÃ©rifier le nombre d'entreprises : `Entreprise.objects.count()`
- [ ] VÃ©rifier les ProLocalisations : `ProLocalisation.objects.count()`
- [ ] CrÃ©er les catÃ©gories manquantes : `create_categories_from_insee`
- [ ] CrÃ©er les ProLocalisations manquantes : `create_missing_prolocalisations`
- [ ] GÃ©nÃ©rer le contenu IA : `generate_ai_reviews_v2`
- [ ] VÃ©rifier les indexes : `\d+ enterprise_entreprise`
- [ ] Backup de la base de donnÃ©es
- [ ] Tester la recherche sur le frontend

---

## ğŸ¯ Prochaines Ã‰tapes

1. **Import complet** (3-6h)
2. **CrÃ©er catÃ©gories** (30 min)
3. **CrÃ©er ProLocalisations** (1-2h)
4. **GÃ©nÃ©rer contenu IA** (10-20h en arriÃ¨re-plan)
5. **Optimiser recherche** (indexes, cache)

**Objectif Final** : 10M+ entreprises, 50M+ ProLocalisations, 100% de couverture France ğŸ‡«ğŸ‡·
