# ðŸ“¦ Guide d'Import : 4 Millions d'Entreprises

## ðŸŽ¯ StratÃ©gie d'Import

### PrÃ©requis CRITIQUES
- âœ… **Cursor pagination activÃ©e** ([enterprise/api/views.py](foxreviews/enterprise/api/views.py#L36))
- âœ… **Indexes SQL crÃ©Ã©s** (exÃ©cuter [SCALING_4M_ENTREPRISES.sql](SCALING_4M_ENTREPRISES.sql))
- â³ **~7.5GB espace disque libre** (table + indexes)
- â³ **16GB+ RAM serveur PostgreSQL** recommandÃ©
- â³ **Celery workers actifs** pour traitement asynchrone

---

## ðŸ“‹ Plan d'Import Progressif

### Phase 1 : Import par Batches (6-12 heures estimÃ©es)

```python
# foxreviews/core/management/commands/import_entreprises_bulk.py

from django.core.management.base import BaseCommand
from django.db import transaction
from foxreviews.enterprise.models import Entreprise
import csv

class Command(BaseCommand):
    help = "Import massif d'entreprises par batches"

    def add_arguments(self, parser):
        parser.add_argument('csv_file', type=str, help='Chemin vers le CSV')
        parser.add_argument('--batch-size', type=int, default=1000, help='Taille des batches')
        parser.add_argument('--max-rows', type=int, help='Limite pour tests')

    def handle(self, *args, **options):
        csv_file = options['csv_file']
        batch_size = options['batch_size']
        max_rows = options.get('max_rows')
        
        batch = []
        total = 0
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                if max_rows and total >= max_rows:
                    break
                
                # CrÃ©er instance (sans save)
                entreprise = Entreprise(
                    siren=row['siren'],
                    siret=row.get('siret', ''),
                    nom=row['nom'],
                    nom_commercial=row.get('nom_commercial', ''),
                    adresse=row['adresse'],
                    code_postal=row['code_postal'],
                    ville_nom=row['ville_nom'],
                    naf_code=row['naf_code'],
                    naf_libelle=row.get('naf_libelle', ''),
                    telephone=row.get('telephone', ''),
                    email_contact=row.get('email', ''),
                    site_web=row.get('site_web', ''),
                    is_active=True,
                )
                batch.append(entreprise)
                total += 1
                
                # Bulk create par batch
                if len(batch) >= batch_size:
                    Entreprise.objects.bulk_create(
                        batch, 
                        batch_size=batch_size,
                        ignore_conflicts=True  # Ignore doublons SIREN
                    )
                    self.stdout.write(f"âœ… {total:,} entreprises importÃ©es...")
                    batch = []
            
            # Dernier batch
            if batch:
                Entreprise.objects.bulk_create(
                    batch, 
                    batch_size=batch_size,
                    ignore_conflicts=True
                )
        
        self.stdout.write(self.style.SUCCESS(f"âœ… Import terminÃ©: {total:,} entreprises"))
```

### Commande d'Import

```bash
# Test avec 10K entreprises d'abord
python manage.py import_entreprises_bulk data/entreprises.csv --batch-size 1000 --max-rows 10000

# Import complet (6-12h selon hardware)
python manage.py import_entreprises_bulk data/entreprises.csv --batch-size 1000
```

---

## âš¡ Optimisations pendant l'Import

### 1. DÃ©sactiver temporairement les contraintes

```sql
-- AVANT import
ALTER TABLE enterprise_entreprise DISABLE TRIGGER ALL;

-- APRÃˆS import
ALTER TABLE enterprise_entreprise ENABLE TRIGGER ALL;
```

### 2. Augmenter les paramÃ¨tres PostgreSQL

```ini
# postgresql.conf (temporaire pendant import)
maintenance_work_mem = 2GB          # Pour crÃ©ation indexes
work_mem = 256MB                    # Pour tris
shared_buffers = 4GB                # Cache PostgreSQL
max_wal_size = 4GB                  # WAL pendant import
checkpoint_timeout = 30min          # Moins de checkpoints
```

### 3. CrÃ©er les indexes APRÃˆS l'import

```sql
-- Import d'abord SANS index (plus rapide)
-- Puis crÃ©er les index:
CREATE INDEX CONCURRENTLY ...  -- N'impacte pas les requÃªtes
```

---

## ðŸ“Š Monitoring pendant l'Import

### 1. Taille de la base
```sql
SELECT 
    pg_size_pretty(pg_database_size('foxreviews_db')) AS db_size,
    pg_size_pretty(pg_total_relation_size('enterprise_entreprise')) AS table_size;
```

### 2. Progression de l'import
```sql
SELECT COUNT(*) FROM enterprise_entreprise;
-- Comparer avec total attendu: 4,000,000
```

### 3. Performance des requÃªtes
```sql
SELECT 
    query,
    calls,
    total_time,
    mean_time
FROM pg_stat_statements
WHERE query LIKE '%enterprise_entreprise%'
ORDER BY total_time DESC
LIMIT 10;
```

---

## ðŸŽ¯ AprÃ¨s l'Import

### 1. VÃ©rification des donnÃ©es
```python
from foxreviews.enterprise.models import Entreprise

# Total
print(f"Total: {Entreprise.objects.count():,}")

# RÃ©partition par dÃ©partement
top_deps = Entreprise.objects.values('code_postal__startswith=75')\
    .annotate(count=Count('id'))\
    .order_by('-count')[:10]
```

### 2. Test de performance
```python
from django.db import connection
from django.test.utils import CaptureQueriesContext

# Test cursor pagination
with CaptureQueriesContext(connection) as ctx:
    entreprises = list(Entreprise.objects.all()[:20])
    print(f"Queries: {len(ctx)}, Time: {sum(float(q['time']) for q in ctx)}s")
    # Attendu: 1 query, < 50ms

# Test recherche
with CaptureQueriesContext(connection) as ctx:
    results = list(Entreprise.objects.filter(nom__icontains='restaurant')[:20])
    print(f"Queries: {len(ctx)}, Time: {sum(float(q['time']) for q in ctx)}s")
    # Attendu: 1 query, < 100ms avec index GIN
```

### 3. Maintenance
```sql
-- Analyser pour optimiser query planner
ANALYZE enterprise_entreprise;

-- Statistiques Ã©tendues (optionnel)
VACUUM ANALYZE enterprise_entreprise;
```

---

## âš ï¸ ProblÃ¨mes Potentiels

### ProblÃ¨me 1 : Import trop lent
**Solution** : 
- Augmenter `batch_size` Ã  5000-10000
- DÃ©sactiver triggers temporairement
- CrÃ©er indexes APRÃˆS import

### ProblÃ¨me 2 : Manque d'espace disque
**Solution** :
- 4M Ã— 1KB = ~4GB table
- + 3.5GB indexes
- **PrÃ©voir 10GB minimum**

### ProblÃ¨me 3 : Queries lentes aprÃ¨s import
**Solution** :
- VÃ©rifier que tous les index sont crÃ©Ã©s
- ExÃ©cuter `ANALYZE`
- VÃ©rifier avec `EXPLAIN ANALYZE`

---

## ðŸ“ˆ Objectifs de Performance

| OpÃ©ration | Objectif | Acceptable | âš ï¸ ProblÃ¨me |
|-----------|----------|------------|-------------|
| Liste page 1 | < 50ms | < 100ms | > 200ms |
| Recherche nom | < 100ms | < 200ms | > 500ms |
| Filtre ville + NAF | < 150ms | < 300ms | > 500ms |
| Import 1000 rows | < 2s | < 5s | > 10s |

---

## ðŸš€ Next Steps : Elasticsearch

Quand les requÃªtes dÃ©passent 200ms en moyenne :
1. Installer Elasticsearch
2. Indexer les entreprises
3. Basculer les recherches vers ES
4. Garder PostgreSQL pour CRUD

**Gain attendu** : 200ms â†’ 20-50ms sur recherches complexes
