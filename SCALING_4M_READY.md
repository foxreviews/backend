# ğŸ¯ PLAN DE SCALING : 4 Millions d'Entreprises + 39K Villes

## âœ… MODIFICATIONS APPLIQUÃ‰ES

### 1. Cursor Pagination ActivÃ©e âœ…
**Fichier modifiÃ©** : [foxreviews/enterprise/api/views.py](foxreviews/enterprise/api/views.py#L36)

```python
# AVANT: pagination_class = ResultsPageNumberPagination  # âŒ Lent sur 4M
# APRÃˆS: pagination_class = EnterpriseCursorPagination   # âœ… Performance constante
```

**Impact** :
- Page 1 : 50ms â†’ 30ms
- Page 10,000 : 5-10s â†’ **30ms** (constante)
- Pas de COUNT(*) â†’ Ã©conomise 3-5s par requÃªte

---

### 2. Indexes SQL CrÃ©Ã©s âœ…
**Fichier** : [SCALING_4M_ENTREPRISES.sql](SCALING_4M_ENTREPRISES.sql)

**Index critiques ajoutÃ©s** :
- âœ… `enterprise_entreprise_created_id_idx` - Pour cursor pagination par date
- âœ… `enterprise_entreprise_nom_id_idx` - Pour cursor pagination par nom
- âœ… `enterprise_entreprise_nom_trgm_idx` - Pour recherche full-text (nom)
- âœ… `enterprise_entreprise_ville_naf_idx` - Pour filtres frÃ©quents
- âœ… `enterprise_entreprise_cp_active_idx` - Pour entreprises actives par dÃ©partement

**Ã€ exÃ©cuter** :
```bash
psql -U postgres -d foxreviews_db -f SCALING_4M_ENTREPRISES.sql
```

---

### 3. Commande Import OptimisÃ©e âœ…
**Fichier** : [foxreviews/enterprise/management/commands/import_entreprises_bulk.py](foxreviews/enterprise/management/commands/import_entreprises_bulk.py)

**Utilisation** :
```bash
# Test avec 10K entreprises
python manage.py import_entreprises_bulk data/entreprises.csv --batch-size 1000 --max-rows 10000

# Import complet (6-12h estimÃ©)
python manage.py import_entreprises_bulk data/entreprises.csv --batch-size 1000
```

**FonctionnalitÃ©s** :
- âœ… Bulk insert par batches de 1000
- âœ… Gestion des erreurs et doublons
- âœ… ETA et statistiques temps rÃ©el
- âœ… Mode dry-run pour tests
- âœ… Reprise aprÃ¨s interruption (--skip-rows)

---

## ğŸ“Š CAPACITÃ‰ ATTENDUE

### Avec les Optimisations AppliquÃ©es

| Dataset | Performance | Ã‰tat |
|---------|-------------|------|
| **39K villes** | 5-8ms (GIN index) | âœ… Excellent |
| **4M entreprises** | 30-50ms (cursor + index) | âœ… Bon |
| **Recherche full-text** | 50-200ms (GIN trigram) | âœ… Acceptable |
| **Filtre ville + NAF** | 30-100ms (index composite) | âœ… Bon |

### Espace Disque Requis

```
Table entreprises:        ~4.0 GB
Indexes B-tree:          ~2.0 GB
Index GIN trigram:       ~1.5 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                   ~7.5 GB
```

**Recommandation** : PrÃ©voir **10GB minimum** d'espace libre

---

## ğŸš€ PROCÃ‰DURE D'IMPORT COMPLÃˆTE

### Phase 1 : PrÃ©paration (30 min)

```bash
# 1. CrÃ©er les index AVANT l'import
psql -U postgres -d foxreviews_db -f SCALING_4M_ENTREPRISES.sql

# 2. VÃ©rifier l'espace disque
df -h /var/lib/postgresql/  # Linux
# ou PowerShell: Get-PSDrive C | Select-Object Used,Free

# 3. Test avec Ã©chantillon
python manage.py import_entreprises_bulk data/entreprises.csv --batch-size 1000 --max-rows 10000 --dry-run
```

### Phase 2 : Import (6-12h)

```bash
# Lancer l'import complet
nohup python manage.py import_entreprises_bulk data/entreprises.csv --batch-size 1000 > import.log 2>&1 &

# Suivre la progression
tail -f import.log
```

### Phase 3 : VÃ©rification (15 min)

```sql
-- 1. VÃ©rifier le count
SELECT COUNT(*) FROM enterprise_entreprise;
-- Attendu: 4,000,000

-- 2. Analyser les statistiques
ANALYZE enterprise_entreprise;

-- 3. VÃ©rifier les index
SELECT 
    indexrelname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS size
FROM pg_stat_user_indexes
WHERE relname = 'enterprise_entreprise'
ORDER BY pg_relation_size(indexrelid) DESC;

-- 4. Taille totale
SELECT 
    pg_size_pretty(pg_total_relation_size('enterprise_entreprise')) AS total,
    pg_size_pretty(pg_relation_size('enterprise_entreprise')) AS table,
    pg_size_pretty(pg_indexes_size('enterprise_entreprise')) AS indexes;
```

### Phase 4 : Test Performance (15 min)

```python
# Django shell
from foxreviews.enterprise.models import Entreprise
from django.db import connection
from django.test.utils import CaptureQueriesContext

# Test 1: Cursor pagination
with CaptureQueriesContext(connection) as ctx:
    list(Entreprise.objects.all()[:20])
    print(f"Pagination: {len(ctx)} queries, {sum(float(q['time']) for q in ctx)*1000:.1f}ms")
    # Attendu: 1 query, < 50ms

# Test 2: Recherche full-text
with CaptureQueriesContext(connection) as ctx:
    list(Entreprise.objects.filter(nom__icontains='restaurant')[:20])
    print(f"Recherche: {len(ctx)} queries, {sum(float(q['time']) for q in ctx)*1000:.1f}ms")
    # Attendu: 1 query, < 200ms

# Test 3: Filtre combinÃ©
with CaptureQueriesContext(connection) as ctx:
    list(Entreprise.objects.filter(ville_nom='Paris', naf_code__startswith='62')[:20])
    print(f"Filtre: {len(ctx)} queries, {sum(float(q['time']) for q in ctx)*1000:.1f}ms")
    # Attendu: 1 query, < 100ms
```

---

## âš ï¸ POINTS DE VIGILANCE

### 1. Performance PostgreSQL

Si les requÃªtes dÃ©passent **200ms** en moyenne :

#### Solution ImmÃ©diate
```sql
-- VÃ©rifier que les index sont utilisÃ©s
EXPLAIN ANALYZE 
SELECT * FROM enterprise_entreprise 
WHERE nom ILIKE '%restaurant%' 
LIMIT 20;
-- Doit voir "Index Scan using enterprise_entreprise_nom_trgm_idx"
```

#### Solution Ã  Moyen Terme : Elasticsearch
- Installation : Docker ou service managÃ©
- Indexation des 4M entreprises : ~30 min
- Gain : 200ms â†’ 20-50ms sur recherches complexes

### 2. MÃ©moire Serveur

**Minimum recommandÃ©** :
- **4GB RAM** : Fonctionne mais lent
- **8GB RAM** : Bon pour dev/test
- **16GB RAM** : RecommandÃ© pour production
- **32GB+ RAM** : IdÃ©al pour 4M+ entreprises

**Configuration PostgreSQL** :
```ini
# postgresql.conf
shared_buffers = 2GB              # 25% de la RAM
effective_cache_size = 6GB        # 75% de la RAM
maintenance_work_mem = 512MB
work_mem = 64MB
```

### 3. Backup et Maintenance

```bash
# Backup avant import (sÃ©curitÃ©)
pg_dump -U postgres foxreviews_db > backup_pre_import.sql

# Maintenance hebdomadaire
VACUUM ANALYZE enterprise_entreprise;

# Rebuild index si fragmentÃ© (aprÃ¨s 6 mois)
REINDEX TABLE CONCURRENTLY enterprise_entreprise;
```

---

## ğŸ“ˆ Ã‰VOLUTION FUTURE

### Quand migrer vers Elasticsearch ?

| Indicateur | Seuil | Action |
|------------|-------|--------|
| Recherche > 200ms | Moyenne sur 24h | âš ï¸ Ã‰valuer ES |
| Recherche > 500ms | Pics frÃ©quents | ğŸ”´ Migrer vers ES |
| Load DB > 80% | CPU constant | ğŸ”´ Read replicas + ES |
| Croissance +1M/an | PrÃ©vision | âš ï¸ Planifier ES |

### Roadmap Scaling

```
Maintenant : 4M entreprises
â”œâ”€â”€ âœ… Cursor pagination
â”œâ”€â”€ âœ… Index GIN trigram
â””â”€â”€ âœ… Bulk operations

6 mois : 5-6M entreprises
â”œâ”€â”€ â³ Elasticsearch
â”œâ”€â”€ â³ Read replicas
â””â”€â”€ â³ Cache Redis agrÃ©gÃ©

12 mois : 8-10M entreprises
â”œâ”€â”€ â³ Partitionnement table (par dÃ©partement)
â”œâ”€â”€ â³ CDN pour assets
â””â”€â”€ â³ Load balancer multi-rÃ©gion
```

---

## âœ… CHECKLIST AVANT IMPORT

- [ ] Cursor pagination activÃ©e dans EntrepriseViewSet
- [ ] Indexes SQL crÃ©Ã©s (SCALING_4M_ENTREPRISES.sql)
- [ ] Commande import_entreprises_bulk testÃ©e avec 10K lignes
- [ ] 10GB+ espace disque disponible
- [ ] PostgreSQL shared_buffers â‰¥ 2GB
- [ ] Backup base de donnÃ©es effectuÃ©
- [ ] Celery workers actifs (si gÃ©nÃ©ration IA)
- [ ] Monitoring activÃ© (pg_stat_statements)

---

## ğŸ“ SUPPORT

En cas de problÃ¨me :

1. **Query lente** : VÃ©rifier avec `EXPLAIN ANALYZE`
2. **Import bloquÃ©** : Reprendre avec `--skip-rows`
3. **Manque mÃ©moire** : RÃ©duire `batch_size` Ã  500
4. **Index manquant** : RÃ©exÃ©cuter SCALING_4M_ENTREPRISES.sql

**Logs utiles** :
- `logs/import.log` - Progression import
- `postgresql.log` - RequÃªtes lentes
- `celery.log` - TÃ¢ches asynchrones
